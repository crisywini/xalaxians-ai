{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/cristian.sanchezp/.cache/huggingface/datasets/hakurei___json/hakurei--open-instruct-v1-00713eb9aefc6002/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198451</th>\n",
       "      <td>SELECT customer_id, customer_name, customer_sa...</td>\n",
       "      <td>Write a SQL query to find the customer id, nam...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303426</th>\n",
       "      <td>example_animal = Animal(name=\"Fido\", species=\"...</td>\n",
       "      <td>Create an example instance of the given class.</td>\n",
       "      <td>Class: Animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319200</th>\n",
       "      <td>Photosynthesis is a process in which plants ma...</td>\n",
       "      <td>Explain the process of photosynthesis in a sim...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330086</th>\n",
       "      <td>Product</td>\n",
       "      <td>Find the top 10 most popular products in this ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457858</th>\n",
       "      <td>It's always a good idea to start by identifyin...</td>\n",
       "      <td>Hello, do you have an idea for how i should st...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321892</th>\n",
       "      <td>Once upon a time, there was a lonely fruit bat...</td>\n",
       "      <td>Edit the following paragraph to improve senten...</td>\n",
       "      <td>Once upon time there was a lonely fruit bat, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452291</th>\n",
       "      <td>Bonjour,\\n\\nJe suis ravi de vous aider en tant...</td>\n",
       "      <td>Can you please produce an output with the foll...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68448</th>\n",
       "      <td>\"Plant\"</td>\n",
       "      <td>Given a string, predict which category it belo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491374</th>\n",
       "      <td>Psychological treatments for depression are tr...</td>\n",
       "      <td>Tell me about psychological treatments for dep...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137578</th>\n",
       "      <td>Strength training offers numerous benefits for...</td>\n",
       "      <td>Imagine you are a personal trainer advising a ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6213</th>\n",
       "      <td>Option 1: Vegetarian Stir Fry. Ingredients: 1/...</td>\n",
       "      <td>Provide two inputs to the instruction \"Generat...</td>\n",
       "      <td>Option 1: Vegetarian \\nOption 2: Low-calorie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450519</th>\n",
       "      <td>Zero point and color correction are important ...</td>\n",
       "      <td>how do I do zero point and color correction fo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62142</th>\n",
       "      <td>My favorite food is pizza. I love the taste of...</td>\n",
       "      <td>Create a paragraph about your favorite food.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184270</th>\n",
       "      <td>Some popular books on mindfulness that you may...</td>\n",
       "      <td>What books can I read to learn more about mind...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120250</th>\n",
       "      <td>\"dog\", \"domesticated\", \"animal\"</td>\n",
       "      <td>Find the first occurrence of each word in the ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267527</th>\n",
       "      <td>- Tyrannosaurus rex\\n- Triceratops\\n- Stegosau...</td>\n",
       "      <td>Generate a list of 5 types of dinosaur</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65892</th>\n",
       "      <td>Medical</td>\n",
       "      <td>Classify whether the given text contains medic...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118672</th>\n",
       "      <td>I like to eat apples.\\nI like to eat apples, o...</td>\n",
       "      <td>All sentences must end with a period</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493021</th>\n",
       "      <td>A monetization strategy typically focuses on g...</td>\n",
       "      <td>How does a Monetization strategy differ from a...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157299</th>\n",
       "      <td>The rules and regulations for filing for unemp...</td>\n",
       "      <td>What are the rules and regulations for filing ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "198451  SELECT customer_id, customer_name, customer_sa...   \n",
       "303426  example_animal = Animal(name=\"Fido\", species=\"...   \n",
       "319200  Photosynthesis is a process in which plants ma...   \n",
       "330086                                            Product   \n",
       "457858  It's always a good idea to start by identifyin...   \n",
       "321892  Once upon a time, there was a lonely fruit bat...   \n",
       "452291  Bonjour,\\n\\nJe suis ravi de vous aider en tant...   \n",
       "68448                                             \"Plant\"   \n",
       "491374  Psychological treatments for depression are tr...   \n",
       "137578  Strength training offers numerous benefits for...   \n",
       "6213    Option 1: Vegetarian Stir Fry. Ingredients: 1/...   \n",
       "450519  Zero point and color correction are important ...   \n",
       "62142   My favorite food is pizza. I love the taste of...   \n",
       "184270  Some popular books on mindfulness that you may...   \n",
       "120250                    \"dog\", \"domesticated\", \"animal\"   \n",
       "267527  - Tyrannosaurus rex\\n- Triceratops\\n- Stegosau...   \n",
       "65892                                             Medical   \n",
       "118672  I like to eat apples.\\nI like to eat apples, o...   \n",
       "493021  A monetization strategy typically focuses on g...   \n",
       "157299  The rules and regulations for filing for unemp...   \n",
       "\n",
       "                                              instruction  \\\n",
       "198451  Write a SQL query to find the customer id, nam...   \n",
       "303426     Create an example instance of the given class.   \n",
       "319200  Explain the process of photosynthesis in a sim...   \n",
       "330086  Find the top 10 most popular products in this ...   \n",
       "457858  Hello, do you have an idea for how i should st...   \n",
       "321892  Edit the following paragraph to improve senten...   \n",
       "452291  Can you please produce an output with the foll...   \n",
       "68448   Given a string, predict which category it belo...   \n",
       "491374  Tell me about psychological treatments for dep...   \n",
       "137578  Imagine you are a personal trainer advising a ...   \n",
       "6213    Provide two inputs to the instruction \"Generat...   \n",
       "450519  how do I do zero point and color correction fo...   \n",
       "62142        Create a paragraph about your favorite food.   \n",
       "184270  What books can I read to learn more about mind...   \n",
       "120250  Find the first occurrence of each word in the ...   \n",
       "267527             Generate a list of 5 types of dinosaur   \n",
       "65892   Classify whether the given text contains medic...   \n",
       "118672               All sentences must end with a period   \n",
       "493021  How does a Monetization strategy differ from a...   \n",
       "157299  What are the rules and regulations for filing ...   \n",
       "\n",
       "                                                    input  \n",
       "198451                                                     \n",
       "303426                                      Class: Animal  \n",
       "319200                                                     \n",
       "330086                                                     \n",
       "457858                                                     \n",
       "321892  Once upon time there was a lonely fruit bat, J...  \n",
       "452291                                                     \n",
       "68448                                                      \n",
       "491374                                                     \n",
       "137578                                                     \n",
       "6213         Option 1: Vegetarian \\nOption 2: Low-calorie  \n",
       "450519                                                     \n",
       "62142                                                      \n",
       "184270                                                     \n",
       "120250                                                     \n",
       "267527                                                     \n",
       "65892                                                      \n",
       "118672                                                     \n",
       "493021                                                     \n",
       "157299                                                     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('hakurei/open-instruct-v1', split='train')\n",
    "dataset.to_pandas().sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    example['prompt'] = f\"{example['instruction']} {example['input']} {example['output']}\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\cristian.sanchezp\\.cache\\huggingface\\datasets\\hakurei___json\\hakurei--open-instruct-v1-00713eb9aefc6002\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-cbda1d7d0dbdd8ca.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=['instruction','input','output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46534</th>\n",
       "      <td>Write an opening statement for a presentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139015</th>\n",
       "      <td>You are given the dimensions and the weight of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63101</th>\n",
       "      <td>Tell me what happened on &lt;date&gt; in history.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214874</th>\n",
       "      <td>How can I be healthy ?  Being healthy is not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365087</th>\n",
       "      <td>How do you think about the future of technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301132</th>\n",
       "      <td>Based on the provided text, edit the mistakes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253056</th>\n",
       "      <td>When was West Ham started and when did they ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372138</th>\n",
       "      <td>Count how many vowels are there in a given str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199846</th>\n",
       "      <td>Write a Java program that adds two numbers and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370991</th>\n",
       "      <td>Can you write a program that can identify if t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt\n",
       "46534   Write an opening statement for a presentation ...\n",
       "139015  You are given the dimensions and the weight of...\n",
       "63101   Tell me what happened on <date> in history.\\n\\...\n",
       "214874  How can I be healthy ?  Being healthy is not a...\n",
       "365087  How do you think about the future of technolog...\n",
       "301132  Based on the provided text, edit the mistakes....\n",
       "253056  When was West Ham started and when did they ge...\n",
       "372138  Count how many vowels are there in a given str...\n",
       "199846  Write a Java program that adds two numbers and...\n",
       "370991  Can you write a program that can identify if t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\cristian.sanchezp\\.cache\\huggingface\\datasets\\hakurei___json\\hakurei--open-instruct-v1-00713eb9aefc6002\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-2bc5d54f439219b6.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42).select(range(100000)).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/DialoGPT-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return dataset.map(lambda example: tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c545b6e656f24108b4697bc29f0faef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3718138ba19467e972cf8414e93e15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = tokenize_dataset(train_dataset)\n",
    "test_dataset = tokenize_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer will use this to process the input and create appropiate batches for training, as we use the generative model, the mlm will be set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../../data/interim/',\n",
    "    num_train_epochs=1, #To keep things fast\n",
    "    per_device_eval_batch_size=4,\n",
    "    per_device_train_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will do all the heavy lifting\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=test_dataset, \n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52440de1e44a28853c7b321a13b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 408488896 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1554\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   1555\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1556\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[0;32m   1557\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1558\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\transformers\\trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1834\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1835\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1837\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1838\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1839\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1840\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1841\u001b[0m ):\n\u001b[0;32m   1842\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2676\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2678\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2679\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2681\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2682\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\transformers\\trainer.py:2704\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2704\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2705\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2706\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\xalaxians_env\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1105\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1103\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   1104\u001b[0m \u001b[39m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m shift_logits \u001b[39m=\u001b[39m lm_logits[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m   1106\u001b[0m shift_labels \u001b[39m=\u001b[39m labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m   1107\u001b[0m \u001b[39m# Flatten the tokens\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 408488896 bytes."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('xalaxians_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b94f8552009d7fcb73af2742ecfa37a7f8bbb6f8e5c677622085453cebaad380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
